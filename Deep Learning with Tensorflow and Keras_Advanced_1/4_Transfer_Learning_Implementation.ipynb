{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "917c0a3e-3a81-40bd-ac3f-a548ec8fbb34",
   "metadata": {},
   "source": [
    "# Transfer Learning Implementation \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f3ed27-f588-491b-9997-dbbeb70779ed",
   "metadata": {},
   "source": [
    "### Step-by-Step Guide: \n",
    "\n",
    "#### Step 1: Setup the Environment \n",
    "\n",
    "Before we start, make sure to import the required libraries: TensorFlow and Keras. Keras is included within TensorFlow as `tensorflow.keras`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769b7642-9ffa-496f-a22b-900f7ebcbad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.16.2 matplotlib==3.9.1\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135bbd47-c7b2-4e71-968e-200fbe7b8cc9",
   "metadata": {},
   "source": [
    "##### Explanation:\n",
    "- `tensorflow` is the main library for machine learning in Python.\n",
    "- `Sequential` is used to create a model with a linear stack of layers.\n",
    "- `Dense` and `Flatten` are types of layers that we will use in our model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fa4e01-165a-4387-aace-da7fd56ce78c",
   "metadata": {},
   "source": [
    "#### Step 2: Load Pre-trained Model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4e5fb4-7b2b-4487-8cf7-94a2a69aefc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the VGG16 model pre-trained on ImageNet\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze the base model layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b4ac35-5359-4c74-b7bf-8d4b72007fbb",
   "metadata": {},
   "source": [
    "#### Step 3: Create and Compile the Model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab87715-f69e-4889-a306-40fe809988b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new model and add the base model and new layers\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # Change to the number of classes you have\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75719b73-022a-479d-8c48-6389a76442a2",
   "metadata": {},
   "source": [
    "### **Create Placeholder Images**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc34894-ddc4-47f0-a325-34709db0b39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs('sample_data/class_a', exist_ok=True)\n",
    "os.makedirs('sample_data/class_b', exist_ok=True)\n",
    "\n",
    "# Create 10 sample images for each class\n",
    "for i in range(10):\n",
    "    # Create a blank white image for class_a\n",
    "    img = Image.fromarray(np.ones((224, 224, 3), dtype=np.uint8) * 255)\n",
    "    img.save(f'sample_data/class_a/img_{i}.jpg')\n",
    "\n",
    "    # Create a blank black image for class_b\n",
    "    img = Image.fromarray(np.zeros((224, 224, 3), dtype=np.uint8))\n",
    "    img.save(f'sample_data/class_b/img_{i}.jpg')\n",
    "\n",
    "print(\"Sample images created in 'sample_data/'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0bb8c1-1073-414e-bae2-8ba3ad029c08",
   "metadata": {},
   "source": [
    "#### Step 4: Train the Model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d62928-699b-465f-a487-e3ab61284fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the dataset\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    'sample_data',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "# Verify if the generator has loaded images correctly\n",
    "print(f\"Found {train_generator.samples} images belonging to {train_generator.num_classes} classes.\")\n",
    "\n",
    "# Train the model\n",
    "if train_generator.samples > 0:\n",
    "    model.fit(train_generator, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0924e6-440a-4f04-b917-24f16c7bc643",
   "metadata": {},
   "source": [
    "#### Step 5: Fine-Tune the Model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c54f44f-a7e4-4f31-ae94-c02d8c809fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze the top layers of the base model \n",
    "\n",
    "for layer in base_model.layers[-4:]:\n",
    "    layer.trainable = True \n",
    "\n",
    "# Compile the model again \n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) \n",
    "\n",
    "# Train the model again \n",
    "model.fit(train_generator, epochs=10) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d5e7f8-93b3-466e-aecc-2e51d8a07c35",
   "metadata": {},
   "source": [
    "### Exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200ce10c-d5da-4972-8c6e-8cfaf7056c0a",
   "metadata": {},
   "source": [
    "#### Exercise 1: Visualize Training and Validation Loss\n",
    "\n",
    "**Objective:** Plot the training and validation loss to observe the learning process of the model.\n",
    "\n",
    "**Instructions:**\n",
    "1. Modify the training code to include validation data.\n",
    "2. Plot the training and validation loss for each epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe25af5-af7e-4398-8474-eb91167597ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf1e5eea-ce2d-4090-9b23-186f3d5ed697",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click here for solution</summary> </br>\n",
    "\n",
    "```python\n",
    "# Modify data generator to include validation data\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    'sample_data',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='binary',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    'sample_data',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='binary',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "# Train the model with validation data\n",
    "history = model.fit(train_generator, epochs=10, validation_data=validation_generator)\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490266d9-6dd7-4944-81ce-4615a8cab5f8",
   "metadata": {},
   "source": [
    "#### Exercise 2: Experiment with Different Optimizers\n",
    "\n",
    "**Objective:** Experiment with different optimizers and observe their impact on model performance.\n",
    "\n",
    "**Instructions:**\n",
    "1. Change the optimizer from `adam` to `sgd` and `rmsprop`.\n",
    "2. Retrain the model with each optimizer and compare the accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec077e3-cac6-4f43-955b-8c84ef791e58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db02a223-c9bb-4649-b397-50ae12d92d09",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click here for solution</summary> </br>\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.models import clone_model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to reset the model weights\n",
    "def reset_model(model):\n",
    "    # Clone the model to reset weights\n",
    "    model_clone = clone_model(model)\n",
    "    model_clone.set_weights(model.get_weights())\n",
    "    return model_clone\n",
    "\n",
    "# Prepare to reset the model for each optimizer test\n",
    "initial_model = reset_model(model)  # Assume 'model' is the initial compiled model\n",
    "\n",
    "# Experiment with SGD optimizer\n",
    "sgd_model = reset_model(initial_model)  # Reset model\n",
    "sgd_model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "history_sgd = sgd_model.fit(train_generator, epochs=10, validation_data=validation_generator)\n",
    "\n",
    "# Plot training and validation accuracy for SGD\n",
    "plt.plot(history_sgd.history['accuracy'], label='Training Accuracy SGD')\n",
    "plt.plot(history_sgd.history['val_accuracy'], label='Validation Accuracy SGD')\n",
    "plt.title('Training and Validation Accuracy with SGD')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Experiment with RMSprop optimizer\n",
    "rmsprop_model = reset_model(initial_model)  # Reset model\n",
    "rmsprop_model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "history_rmsprop = rmsprop_model.fit(train_generator, epochs=10, validation_data=validation_generator)\n",
    "\n",
    "# Plot training and validation accuracy for RMSprop\n",
    "plt.plot(history_rmsprop.history['accuracy'], label='Training Accuracy RMSprop')\n",
    "plt.plot(history_rmsprop.history['val_accuracy'], label='Validation Accuracy RMSprop')\n",
    "plt.title('Training and Validation Accuracy with RMSprop')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731c6f21-35e4-4281-9116-0b302c30b000",
   "metadata": {},
   "source": [
    "#### Exercise 3: Evaluate the Model on a Test Set\n",
    "\n",
    "**Objective:** Evaluate the fine-tuned model on an unseen test set to assess its generalization performance.\n",
    "\n",
    "**Instructions:**\n",
    "1. Load a separate test set.\n",
    "2. Evaluate the model on this test set and report the accuracy and loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9580ec-b3f2-4f51-a668-c227406ac16b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea02abc8-07d8-4be1-962f-54d29d692a3a",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click here for solution</summary> </br>\n",
    "\n",
    "```python\n",
    "# Load and preprocess the test dataset\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    'sample_data',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")\n",
    "\n",
    "# Evaluate the fine-tuned model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(test_generator)\n",
    "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
    "print(f'Test Loss: {test_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb74232-a593-45cb-9913-7b3553553dfc",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "By completing these exercises, students will:\n",
    "\n",
    "1. Visualize the training and validation loss to gain insights into the training process.\n",
    "2. Experiment with different optimizers to understand their impact on model performance.\n",
    "3. Evaluate the fine-tuned model on an unseen test set to assess its generalization capability.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "prev_pub_hash": "46890cfd422ab815a33a7c99b85ad21a549fbfa26e2bfd3ec07a5686815da9bc"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
