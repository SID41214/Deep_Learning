{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcf3b4d7-c328-4997-8c9b-a2bf2975e60f",
   "metadata": {},
   "source": [
    "# Practice Project: Fruit Classification Using Transfer Learning \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1619cea-1651-46d2-9bf8-06549df5416b",
   "metadata": {},
   "source": [
    "### Introduction \n",
    "\n",
    "Classify images of fruits using transfer learning with the pre-trained VGG16 model. Transfer learning enables leveraging a model trained on a large dataset (like ImageNet) and applying it to your dataset with fewer data and computational resources.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5851381b-2d02-42da-b477-dcf4a1c049e0",
   "metadata": {},
   "source": [
    "### Aim \n",
    "\n",
    "The aim is to build a fruit image classifier using transfer learning. You will fine-tune a pre-trained model on a custom dataset of fruit images to enable it to classify fruits effectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87ae5c2-74f2-4629-9232-4814a690da76",
   "metadata": {},
   "source": [
    "### Final output \n",
    "\n",
    "The final output will be a trained deep learning model capable of classifying various fruit images into their respective categories. You will also visualize the model's accuracy and predictions on sample test images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00e6702-4533-4615-9bb7-6efc4a83509b",
   "metadata": {},
   "source": [
    "### Learning objectives\n",
    "At the end of the project, you will be able to:\n",
    "- Set up and organize a complex fruit image dataset.\n",
    "- Use transfer learning with the VGG16 model.\n",
    "- Fine-tune a pre-trained model for your dataset.\n",
    "- Evaluate and interpret the model’s performance on unseen data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939c0cf9-a062-475a-820b-0f2107f92f74",
   "metadata": {},
   "source": [
    "### Setup instructions \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1dce0a-6c94-4450-ad6b-12c0da966267",
   "metadata": {},
   "source": [
    "#### Prerequisites \n",
    "\n",
    "- Basic knowledge of Python and Keras. \n",
    "\n",
    "- TensorFlow installed in your Python environment. \n",
    "\n",
    "- A data set of fruit images organized in subdirectories for each class. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266cfcb1-ac21-4bb8-b3e3-8058fabeadec",
   "metadata": {},
   "source": [
    "#### code to suppress warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb594a8-98aa-446b-bd0c-204093ead771",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"keras.src.trainers.data_adapters.py_dataset_adapter\")\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"keras.src.trainers.epoch_iterator\")\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Suppress all warnings and info messages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f15b7e9-eb97-49b3-8ca0-6b27bed5219e",
   "metadata": {},
   "source": [
    "#### Required libraries \n",
    "\n",
    "Install the following libraries, if you haven't already: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9df3be-a63e-4772-8f6f-fa47e7e3dab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.16.2\n",
    "!pip install matplotlib==3.9.2\n",
    "!pip install numpy==1.26.4\n",
    "!pip install scipy==1.14.1\n",
    "!pip install scikit-learn==1.5.2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637016c3-9d2c-426b-a056-282bf667e0b4",
   "metadata": {},
   "source": [
    "### Data preparation \n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1d3e8eb5-227b-49fe-9d1c-4e9b2bcfb2a6",
   "metadata": {},
   "source": [
    "Directory structure \n",
    "\n",
    "Ensure that your data set is organized as follows: \n",
    "\n",
    "\n",
    "dataset/\n",
    "├── train/\n",
    "│   ├── Class1/\n",
    "│   ├── Class2/\n",
    "│   ├── Class3/\n",
    "│   └── (other classes...)\n",
    "├── val/\n",
    "│   ├── Class1/\n",
    "│   ├── Class2/\n",
    "│   ├── Class3/\n",
    "│   └── (other classes...)\n",
    "└── test/\n",
    "    ├── Class1/\n",
    "    ├── Class2/\n",
    "    ├── Class3/\n",
    "    └── (other classes...)\n",
    "\n",
    "\n",
    "Each subdirectory under train and val should contain images of the respective fruit category. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d0c8ec-5106-4245-9afe-b0f0b1ecf459",
   "metadata": {},
   "source": [
    "### Tasks List\n",
    "To achieve the above objectives, you will complete the following tasks:\n",
    "\n",
    "- Task 1: Import necessary libraries and set dataset paths\n",
    "- Task 2: Set up data generators for training, validation, and testing with augmentation\n",
    "- Task 3: Define the VGG16-based model architecture with custom layers\n",
    "- Task 4: Compile the model with appropriate loss and optimizer\n",
    "- Task 5: Train the model with early stopping and learning rate scheduling\n",
    "- Task 6: Fine-tune the model by unfreezing specific layers in VGG16\n",
    "- Task 7: Evaluate the model on the test set and display accuracy\n",
    "- Task 8: Visualize training performance with accuracy and loss curves\n",
    "- Task 9: Test model predictions on sample images and visualize results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cb020c-214a-4b53-bfa2-385a4d81cec6",
   "metadata": {},
   "source": [
    "<h2> Download the input data file </h2>\n",
    "\n",
    "Note: The dataset download may take up to 30 minutes depending on your internet connection. Please ensure a stable connection and wait until the download completes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfe9784-7971-4795-aadf-2519df427b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import zipfile\n",
    "\n",
    "# Define dataset URL and paths\n",
    "url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/4yIRGlIpNfKEGJYMhZV52g/fruits-360-original-size.zip\"\n",
    "local_zip = \"fruits-360-original-size.zip\"\n",
    "extract_dir = \"fruits-360-original-size\"\n",
    "\n",
    "def download_dataset(url, output_file):\n",
    "    \"\"\"Download the dataset using wget in quiet mode.\"\"\"\n",
    "    print(\"Downloading the dataset...\")\n",
    "    subprocess.run([\"wget\", \"-q\", \"-O\", output_file, url], check=True)  # Add `-q` for quiet mode\n",
    "    print(\"Download completed.\")\n",
    "\n",
    "def extract_zip_in_chunks(zip_file, extract_to, batch_size=2000):\n",
    "    \"\"\"\n",
    "    Extract a large zip file in chunks to avoid memory bottlenecks.\n",
    "    Processes a specified number of files (batch_size) at a time.\n",
    "    \"\"\"\n",
    "    print(\"Extracting the dataset in chunks...\")\n",
    "    os.makedirs(extract_to, exist_ok=True)  # Ensure the extraction directory exists\n",
    "    \n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "        files = zip_ref.namelist()  # List all files in the archive\n",
    "        total_files = len(files)\n",
    "        \n",
    "        for i in range(0, total_files, batch_size):\n",
    "            batch = files[i:i+batch_size]\n",
    "            for file in batch:\n",
    "                zip_ref.extract(file, extract_to)  # Extract each file in the batch\n",
    "            print(f\"Extracted {min(i+batch_size, total_files)} of {total_files} files...\")\n",
    "    \n",
    "    print(f\"Dataset successfully extracted to '{extract_to}'.\")\n",
    "\n",
    "# Main script execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Download the dataset if not already downloaded\n",
    "    if not os.path.exists(local_zip):\n",
    "        download_dataset(url, local_zip)\n",
    "    else:\n",
    "        print(\"Dataset already downloaded.\")\n",
    "    \n",
    "    # Extract the dataset if not already extracted\n",
    "    if not os.path.exists(extract_dir):\n",
    "        extract_zip_in_chunks(local_zip, extract_dir)\n",
    "    else:\n",
    "        print(\"Dataset already extracted.\")\n",
    "    \n",
    "    # Optional cleanup of the zip file\n",
    "    if os.path.exists(local_zip):\n",
    "        os.remove(local_zip)\n",
    "        print(f\"Cleaned up zip file: {local_zip}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193f0590-4efd-4f79-be45-220c6345b680",
   "metadata": {},
   "source": [
    "<h5>Note: If you see warnings related to GPU (e.g., CUDA or cuDNN), it means the system is running on the CPU. Training may take longer.</h5>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7380b61e-585b-4fa1-a40b-bbd1e584809a",
   "metadata": {},
   "source": [
    "<h3> Task 1: Import necessary libraries and set dataset paths </h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be00f52-f2d5-457e-85f0-78ec2ff3a7f7",
   "metadata": {},
   "source": [
    "**Explanation:** This task involves importing essential libraries and setting up the paths for the dataset directories (train, val, and test). These libraries are necessary for data handling, model building, and performance evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423d558d-b0ca-492c-b582-2760573b0971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "# Set dataset paths\n",
    "train_dir = 'fruits-360-original-size/fruits-360-original-size/Training'\n",
    "val_dir = 'fruits-360-original-size/fruits-360-original-size/Validation'\n",
    "test_dir = 'fruits-360-original-size/fruits-360-original-size/Test'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafd131b-fa77-4116-8e30-2f47dcf273c0",
   "metadata": {},
   "source": [
    "### Library Explanations:\n",
    "- `ImageDataGenerator:` For loading images and applying data augmentation.\n",
    "- `VGG16:` Pre-trained model used for transfer learning.\n",
    "- `Sequential:` For building a sequential model.\n",
    "- `Dense, Flatten, Dropout, BatchNormalization:` Layers to customize the model architecture.\n",
    "- `ReduceLROnPlateau, EarlyStopping:` Callbacks for optimizing training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b903b9-83ed-4a28-abe6-02766cbce88a",
   "metadata": {},
   "source": [
    "<h3> Task 2: Set up data generators for training, validation, and testing with augmentation </h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07984a58-9e44-466e-a8c3-4c62ef55b5e9",
   "metadata": {},
   "source": [
    "**Explanation:** Data generators load images from directories, rescale them, and apply augmentation on the training set to help the model generalize better. Validation and test sets are only rescaled (no augmentation).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77738378-1f08-4461-80e7-1eeeccbb0268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image data generators\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255.0,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "test_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "\n",
    "# Load images from directories\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(64, 64),\n",
    "    batch_size=16,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(64, 64),\n",
    "    batch_size=16,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(64, 64),\n",
    "    batch_size=16,\n",
    "    class_mode='categorical'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc921e6-603b-4fa6-8e7b-cc5610d7de9c",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "- `train_datagen:` Applies rescaling and augmentation (e.g., rotation, zoom) to make the model more robust.\n",
    "- `val_datagen and test_datagen:` Only rescale images for validation/testing.\n",
    "- `flow_from_directory:` Loads images from specified folders into batches for training/validation/testing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb76aba-b0ba-4431-9a58-26463bb26221",
   "metadata": {},
   "source": [
    "<h3>Task 3: Define the VGG16-based model architecture with custom layers</h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ceddb79-6983-4ecd-b660-7a0a244860c3",
   "metadata": {},
   "source": [
    "**Explanation:** This task involves loading the pre-trained VGG16 model (excluding the top layers) and adding custom layers to adapt it to the fruit classification task.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7780c905-9d1f-4109-9198-b5ce94d9063d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, BatchNormalization, Dropout\n",
    "\n",
    "# Load VGG16 with pre-trained weights\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(64, 64, 3))\n",
    "\n",
    "# Freeze the base model layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Build the model\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dense(256, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "    Dense(train_generator.num_classes, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb7020e-add4-40f4-9b41-2334f3e4f6aa",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "- `base_model:` Loads VGG16, excluding its dense layers (`include_top=False`).\n",
    "- `for layer in base_model.layers:` Freezes VGG16 layers to retain pre-trained weights.\n",
    "- Custom layers: Flatten the output, then add dense layers with regularization (Dropout) and normalization (BatchNormalization) to enhance learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87270d53-582e-4ff3-ab9f-664dab00fe7c",
   "metadata": {},
   "source": [
    "<h3>Task 4: Compile the model with appropriate loss and optimizer</h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d4f37c-3797-4d82-afa7-85dc0a11aae9",
   "metadata": {},
   "source": [
    "**Explanation:** Compile the model to specify the loss function, optimizer, and evaluation metric.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20190e52-9cf2-40ee-a7ba-ca6052868e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3d6928-1e07-4969-a5ee-bffaae066517",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "- `categorical_crossentropy:` Used because this is a multi-class classification task.\n",
    "- `adam:` Adaptive learning rate optimizer that helps in faster convergence.\n",
    "- `metrics=['accuracy']:` Tracks model accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0a94bf-9a36-420c-99b3-4cbf15a32c71",
   "metadata": {},
   "source": [
    "<h3>Task 5: Train the model with early stopping and learning rate scheduling</h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5363d80e-175f-4edc-8515-0449aef98509",
   "metadata": {},
   "source": [
    "**Explanation:** Train the model, using callbacks to monitor the validation loss and adjust the learning rate or stop early to prevent overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cd9223-9a66-4d94-8df2-72e289d9cb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.mixed_precision import set_global_policy\n",
    "\n",
    "# Define callbacks\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=1e-6, verbose=1)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Enable mixed precision (if on GPU)\n",
    "set_global_policy('float32')\n",
    "\n",
    "steps_per_epoch = 50 \n",
    "validation_steps = 25\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=5,\n",
    "    validation_data=val_generator,  \n",
    "    steps_per_epoch=steps_per_epoch,  \n",
    "    validation_steps=validation_steps,  \n",
    "    callbacks=[lr_scheduler, early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42745bd0-9e7c-4dda-aac2-6b93f6afcd5c",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "- `ReduceLROnPlateau`: Reduces learning rate when validation loss plateaus, allowing better optimization.\n",
    "- `EarlyStopping`: Stops training when validation loss no longer improves, preventing overfitting.\n",
    "- `model.fit`: Trains the model on the `train_generator` and evaluates on `val_generator` each epoch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d6c41e-6947-47bc-96b1-ed28a993b03e",
   "metadata": {},
   "source": [
    "<h3>Task 6: Fine-tune the model by unfreezing specific layers in VGG16</h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831e152a-91ad-47f7-b6f0-a1504212c811",
   "metadata": {},
   "source": [
    "**Explanation:** Fine-tune by unfreezing a few layers in the VGG16 base model to allow learning on fruit-specific features.\n",
    "\n",
    "**Note**: Fine-tuning may take excess time on a CPU-based machine. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cb9666-8247-431c-b11c-0b91ad05a6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import tensorflow as tf  # Import TensorFlow for accessing tf.keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Check the number of layers in the base model\n",
    "num_layers = len(base_model.layers)\n",
    "print(f\"The base model has {num_layers} layers.\")\n",
    "\n",
    "# Unfreeze the last 5 layers for fine-tuning\n",
    "for layer in base_model.layers[-5:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# Freeze BatchNorm layers to speed up fine-tuning\n",
    "for layer in base_model.layers:\n",
    "    if isinstance(layer, tf.keras.layers.BatchNormalization):\n",
    "        layer.trainable = False\n",
    "\n",
    "# Re-compile the model with a faster optimizer\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),   # Higher learning rate for faster convergence\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Continue training with fewer steps per epoch\n",
    "history_fine = model.fit(\n",
    "    train_generator,\n",
    "    epochs=5,\n",
    "    validation_data=val_generator,\n",
    "    steps_per_epoch=steps_per_epoch,  # Reduced steps per epoch\n",
    "    validation_steps=validation_steps,  # Reduced validation steps\n",
    "    callbacks=[lr_scheduler, early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38698d77-fb46-4238-8066-e12ff8266cd6",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "- `for layer in base_model.layers[-5:]`: Unfreezes the last 5 layers to allow fine-tuning.\n",
    "-  Unfreezing fewer layers is faster and prevents overfitting on small datasets.\n",
    "- `RMSprop(learning_rate=1e-5)`: Optimizer with a lower learning rate to fine-tune carefully without drastic weight changes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379743f4-229e-4136-81dd-7477aef6b084",
   "metadata": {},
   "source": [
    "<h3>Task 7: Evaluate the model on the test set and display accuracy</h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f626609d-62dc-4171-a05a-3190c96c9445",
   "metadata": {},
   "source": [
    "**Explanation**: Evaluates the final model on unseen test data to gauge its generalization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cc85e6-1ae0-458c-aa83-8e6e07c7a43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the test set\n",
    "test_loss, test_accuracy = model.evaluate(test_generator, steps=50)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1da6a9-6d9b-4e0b-b390-778767c842d4",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "\n",
    "- `model.evaluate(test_generator)`: Evaluates the model on the test set and prints accuracy, giving a final measure of model performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f0d4ee-2840-4288-862b-f0dc92988415",
   "metadata": {},
   "source": [
    "<h3> Task 8: Visualize training performance with accuracy and loss curves </h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf97ea10-cd69-4824-93a1-7d09b3cb2b56",
   "metadata": {},
   "source": [
    "**Explanation**: Plots the training and validation accuracy and loss to understand the model’s learning progress.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0196ea39-f769-484d-8d21-cc64dc73ea32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy and loss curves\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.plot(history_fine.history['accuracy'], label='Fine-tuned Training Accuracy')\n",
    "plt.plot(history_fine.history['val_accuracy'], label='Fine-tuned Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.plot(history_fine.history['loss'], label='Fine-tuned Training Loss')\n",
    "plt.plot(history_fine.history['val_loss'], label='Fine-tuned Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35af4921-4edd-470b-84d4-f823ce147352",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "\n",
    "- `plt.plot`: Plots the accuracy and loss for training and validation over epochs.\n",
    "  \n",
    "- Visual comparison shows if the model is overfitting, underfitting, or learning effectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7a0869-5a71-4ece-9d31-c890c62b66ed",
   "metadata": {},
   "source": [
    "<h3>Task 9: Test model predictions on sample images and visualize results</h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f32300-c0cd-40f8-8b7a-d83e1dc50a54",
   "metadata": {},
   "source": [
    "**Explanation:** Makes predictions on a few test images and displays them with the model's predicted class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4cb701-3835-4eae-b87b-dac77317f782",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize counters for actual and predicted classes\n",
    "actual_count = Counter()\n",
    "predicted_count = Counter()\n",
    "\n",
    "# Function to get class name from predicted index\n",
    "def get_class_name_from_index(predicted_index, class_index_mapping):\n",
    "    \"\"\"Convert predicted index to class name.\"\"\"\n",
    "    for class_name, index in class_index_mapping.items():\n",
    "        if index == predicted_index:\n",
    "            return class_name\n",
    "    return \"Unknown\"  # Default if index is not found\n",
    "\n",
    "# Define the function for visualization\n",
    "def visualize_prediction_with_actual(img_path, class_index_mapping):\n",
    "    # Extract the true label dynamically from the directory structure\n",
    "    class_name = os.path.basename(os.path.dirname(img_path))  # Extract folder name (class)\n",
    "    \n",
    "    # Load and preprocess the image\n",
    "    img = load_img(img_path, target_size=(64, 64)) \n",
    "    img_array = img_to_array(img) / 255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "    # Predict the class\n",
    "    prediction = model.predict(img_array)\n",
    "    predicted_index = np.argmax(prediction, axis=-1)[0]\n",
    "    predicted_class_name = get_class_name_from_index(predicted_index, class_index_mapping)\n",
    "\n",
    "    # Update the counters\n",
    "    actual_count[class_name] += 1\n",
    "    predicted_count[predicted_class_name] += 1\n",
    "\n",
    "    # Visualize the image with predictions\n",
    "    plt.figure(figsize=(2, 2), dpi=100)\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"Actual: {class_name}, Predicted: {predicted_class_name}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Retrieve class index mapping from the training generator\n",
    "class_index_mapping = train_generator.class_indices\n",
    "print(\"Class Index Mapping:\", class_index_mapping)  # Debugging: Check the mapping\n",
    "\n",
    "# Define a list of image paths without hardcoded labels\n",
    "sample_images = [\n",
    "    'fruits-360-original-size/fruits-360-original-size/Test/apple_braeburn_1/r0_11.jpg',\n",
    "    'fruits-360-original-size/fruits-360-original-size/Test/pear_1/r0_103.jpg',\n",
    "    'fruits-360-original-size/fruits-360-original-size/Test/cucumber_3/r0_103.jpg',\n",
    "]\n",
    "\n",
    "# Run the predictions and visualization\n",
    "for img_path in sample_images:\n",
    "    visualize_prediction_with_actual(img_path, class_index_mapping)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744f9988-4cad-42f8-9c67-8af75cbb30dd",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "\n",
    "- `visualize_prediction`: Loads an image, preprocesses it, predicts its class, and displays it.\n",
    "  \n",
    "- `model.predict(img_array)`: Uses the trained model to make predictions on unseen images.\n",
    "\n",
    "##### If there is an incorrect prediction during testing, the following factors could contribute to the misclassification:\n",
    "\n",
    "Class Similarity: Visually similar fruit classes (e.g., apple types) can confuse the model.\n",
    "\n",
    "Insufficient Data: Imbalanced datasets with fewer samples for certain classes may cause underfitting.\n",
    "\n",
    "Limited Training: Fine-tuning fewer layers might not capture sufficient class-specific features.\n",
    "\n",
    "Data Augmentation Impact: Aggressive augmentations may distort key features, reducing accuracy for specific images.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "prev_pub_hash": "a5004bd4f77c95c33c59934b2ac2d6d3a45d1dd6869bb6376c595cfaf2a2704d"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
