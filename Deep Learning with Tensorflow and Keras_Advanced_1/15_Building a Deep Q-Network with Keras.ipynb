{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f115b78-065a-42e8-8305-647702730121",
   "metadata": {},
   "source": [
    "# **Building a Deep Q-Network with Keras**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51acf721-b5b3-4969-8fe1-347470871294",
   "metadata": {},
   "source": [
    "## Learning objectives \n",
    "\n",
    "- Implement a Deep Q-Network using Keras  \n",
    "- Define and train a neural network to approximate the Q-values  \n",
    "- Evaluate the performance of the trained DQN agent \n",
    "\n",
    "## Prerequisites \n",
    "- Basic understanding of Python and Keras \n",
    "- Familiarity with neural networks \n",
    "- Understanding of reinforcement learning concepts \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b473ec5-34a0-401c-aa14-e1dc20c02b48",
   "metadata": {},
   "source": [
    "### Steps \n",
    "\n",
    "#### Step 1: Set up the environment \n",
    "\n",
    "Before you start, set up the environment using the OpenAI Gym library. You will use the 'CartPole-v1' environment, which is a common benchmark for reinforcement learning algorithms. \n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff81e42-7d40-49ab-bfe4-b791d9fd7946",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922d2cb6-4819-4f9c-a40d-43aa5a52141b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.16.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cad454d-b4ba-449d-b8b8-b2146424269b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "# Create the environment  \n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "# Set random seed for reproducibility  \n",
    "np.random.seed(42)\n",
    "env.reset(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79744a6-ce74-4a1a-a11b-05218dbe9798",
   "metadata": {},
   "source": [
    "#### Notes: \n",
    "- `gym` is a toolkit for developing and comparing reinforcement learning algorithms.\n",
    "- `CartPole-v1` is an environment where a pole is balanced on a cart, and the goal is to prevent the pole from falling over.  \n",
    "- Setting random seeds ensures that you can reproduce the results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afafe069-acac-46e1-8c2e-da00d4fe69f7",
   "metadata": {},
   "source": [
    "#### Step 2: Define the DQN model \n",
    "\n",
    "Define a neural network using Keras to approximate the Q-values. The network will take the state as input and output Q-values for each action. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdd2bb4-7257-453d-aa1d-4bb5807d5897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings for a cleaner notebook or console experience\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Disable warnings for a cleaner notebook or console experience\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "warnings.warn = warn\n",
    "\n",
    "# Import necessary libraries\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "def build_model(state_size, action_size):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(24, input_dim=state_size, activation='relu'))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(action_size, activation='linear'))\n",
    "    model.compile(loss='mse', optimizer=Adam(learning_rate=0.001))\n",
    "    return model\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "model = build_model(state_size, action_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27957a3a-c8d8-414c-9b04-d0d19f3d1864",
   "metadata": {},
   "source": [
    "#### Notes: \n",
    "\n",
    "- `Sequential` model: a linear stack of layers in Keras. \n",
    "- `Dense` layers: fully connected layers.  \n",
    "- `input_dim`: the size of the input layer, corresponding to the state size.  \n",
    "- `activation='relu'`: Rectified Linear Unit activation function.  \n",
    "- `activation='linear'`: linear activation function for the output layer. \n",
    "- `Adam` optimizer: an optimization algorithm that adjusts the learning rate based on gradients.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f56a25b-40dc-462b-86ae-e92cccbf32d7",
   "metadata": {},
   "source": [
    "#### Step 3: Implement the replay buffer \n",
    "\n",
    "A replay buffer stores the agent's experiences for training. We will implement a replay buffer using a deque. \n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5631e918-4d87-437c-ac8f-45e074b13aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "\n",
    "memory = deque(maxlen=2000)\n",
    "def remember(state, action, reward, next_state, done):\n",
    "    memory.append((state, action, reward, next_state, done))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0e2c27-56f7-4b3d-b778-286af3f94d37",
   "metadata": {},
   "source": [
    "#### Notes: \n",
    "\n",
    "- `memory`: a deque to store experiences (state, action, reward, next_state, done).  \n",
    "- `remember()`: stores experiences in memory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29796f0f-640f-431e-9c81-faeea838f6f3",
   "metadata": {},
   "source": [
    "#### Step 4: Implement the epsilon-greedy policy \n",
    "\n",
    "The epsilon-greedy policy balances exploration and exploitation by choosing random actions with probability epsilon. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d422a6-688f-4822-8016-806e5e63c6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1.0\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.995\n",
    " \n",
    "def act(state):\n",
    "    if np.random.rand() <= epsilon:\n",
    "        return random.randrange(action_size)\n",
    "    q_values = model.predict(state)\n",
    "    return np.argmax(q_values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2ecd53-77ac-42ad-8520-aa53503c8df4",
   "metadata": {},
   "source": [
    "#### Notes: \n",
    "- `epsilon`: exploration rate.  \n",
    "- `epsilon_min`: minimum exploration rate.  \n",
    "- `epsilon_decay`: decay rate for epsilon after each episode.  \n",
    "- `act()`: chooses an action based on the epsilon-greedy policy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b78feb2-79ba-4f34-b224-8a78f0883006",
   "metadata": {},
   "source": [
    "#### Step 5: Implement the Q-learning update \n",
    "\n",
    "Implement the Q-learning update to train the DQN using experiences stored in the replay buffer. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966962dc-2cde-4552-82de-7f22586c52ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replay(batch_size):\n",
    "    global epsilon\n",
    "    minibatch = random.sample(memory, batch_size)\n",
    "    for state, action, reward, next_state, done in minibatch:\n",
    "        target = reward\n",
    "        if not done:\n",
    "            target = reward + gamma * np.amax(model.predict(next_state)[0])\n",
    "        target_f = model.predict(state)\n",
    "        target_f[0][action] = target\n",
    "        model.fit(state, target_f, epochs=1, verbose=0)\n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon *= epsilon_decay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7431dcd6-6c49-47ee-90ec-1fc11560d85a",
   "metadata": {},
   "source": [
    "#### Notes: \n",
    "- `replay()`: samples a random minibatch from memory and trains the model.  \n",
    "- `target`: the Q-value target, which is updated using the reward and the maximum Q-value of the next state.  \n",
    "- `model.fit()`: trains the model on the updated Q-values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb6d6d1-17db-4e33-8641-e5a4a2cf25b8",
   "metadata": {},
   "source": [
    "#### Step 6: Train the DQN \n",
    "\n",
    "Train the DQN agent by interacting with the environment and updating the Q-values using the replay buffer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c52974-207b-40cf-a38c-133cc4525523",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "episodes = 50  # More episodes to ensure sufficient training\n",
    "batch_size = 32  # Mini-batch size for replay training\n",
    "gamma = 0.95  # Discount factor for future rewards\n",
    " \n",
    "for e in range(episodes):\n",
    "    state = env.reset()\n",
    "    if isinstance(state, tuple):  # Handle tuple output\n",
    "        state = state[0]\n",
    "    state = np.reshape(state, [1, state_size])\n",
    " \n",
    "    for time in range(200):  # Max steps per episode\n",
    "        # Choose action using epsilon-greedy policy\n",
    "        action = act(state)\n",
    " \n",
    "        # Perform action in the environment\n",
    "        result = env.step(action)\n",
    "        if len(result) == 4:  # Handle 4-value output\n",
    "            next_state, reward, done, _ = result\n",
    "        else:  # Handle 5-value output\n",
    "            next_state, reward, done, _, _ = result\n",
    " \n",
    "        if isinstance(next_state, tuple):  # Handle tuple next_state\n",
    "            next_state = next_state[0]\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    " \n",
    "        # Store experience in memory\n",
    "        remember(state, action, reward, next_state, done)\n",
    " \n",
    "        # Update state\n",
    "        state = next_state\n",
    " \n",
    "        if done:  # If episode ends\n",
    "            print(f\"Episode: {e+1}/{episodes}, Score: {time}, Epsilon: {epsilon:.2}\")\n",
    "            break\n",
    " \n",
    "    # Train the model using replay memory\n",
    "    if len(memory) > batch_size:\n",
    "        replay(batch_size)\n",
    " \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6687b141-3020-401f-acc0-2fd594ee6e60",
   "metadata": {},
   "source": [
    "#### Notes: \n",
    "- The main loop iterates over episodes, interacting with the environment and training the model.  \n",
    "- `env.reset()`: resets the environment at the beginning of each episode.  \n",
    "- `env.step(action)`: takes the chosen action and observes the reward and next state.  \n",
    "- The score for each episode is printed to monitor training progress.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09a1f4d-26f3-473d-8828-1a8ba4180000",
   "metadata": {},
   "source": [
    "#### Step 7: Evaluate the performance \n",
    "\n",
    "Evaluate the performance of the trained DQN agent. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6171e2ee-fcdc-4ad7-b7b7-079683bdef8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation loop\n",
    "evaluation_episodes = 10  # Number of evaluation episodes\n",
    "scores = []  # Track scores for performance metrics\n",
    " \n",
    "for e in range(evaluation_episodes):\n",
    "    state = env.reset()\n",
    "    if isinstance(state, tuple):  # Handle tuple output\n",
    "        state = state[0]\n",
    "    state = np.reshape(state, [1, state_size])\n",
    " \n",
    "    total_reward = 0  # Track total reward per episode\n",
    " \n",
    "    for time in range(200):  # Max steps per episode\n",
    "        # Choose the greedy action\n",
    "        action = np.argmax(model.predict(state)[0])\n",
    " \n",
    "        # Perform action in the environment\n",
    "        result = env.step(action)\n",
    "        if len(result) == 4:  # Handle 4-value output\n",
    "            next_state, reward, done, _ = result\n",
    "        else:  # Handle 5-value output\n",
    "            next_state, reward, done, _, _ = result\n",
    " \n",
    "        if isinstance(next_state, tuple):  # Handle tuple next_state\n",
    "            next_state = next_state[0]\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    " \n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    " \n",
    "        if done:  # If episode ends\n",
    "            print(f\"Evaluation Episode: {e+1}/{evaluation_episodes}, Score: {time}, Total Reward: {total_reward}\")\n",
    "            scores.append(total_reward)\n",
    "            break\n",
    " \n",
    "# Summary of evaluation performance\n",
    "print(f\"Average Reward: {np.mean(scores):.2f}, Max Reward: {np.max(scores)}, Min Reward: {np.min(scores)}\")\n",
    " \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c36773c-1690-46b1-8a98-2a7bfa1b9c66",
   "metadata": {},
   "source": [
    "#### Notes:\n",
    "- This loop runs 10 episodes to test the trained agent.\n",
    "- `env.render()`: Visualizes the environment.\n",
    "- The agent chooses actions based on the trained model and interacts with the environment.\n",
    "- The environment's `reset` and `step` methods may return additional information in the form of tuples. The code now checks if the `state` and `next_state` are tuples and extracts the necessary data.\n",
    "- The `env.step(action)` method may return more than the standard four values. The code has been updated to handle these additional values by unpacking only the necessary information and ignoring the rest.\n",
    "- The score for each episode is printed, indicating how long the agent was able to balance the pole in each episode.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04d704d-aba4-4dae-846a-e165f777850b",
   "metadata": {},
   "source": [
    "## Practice Exercises\n",
    "### Exercise 1: Modify the Reward Function to Encourage Longer Episodes\n",
    "**Objective:** Modify the reward structure to encourage the agent to keep the pole balanced longer.\n",
    "\n",
    "**Instructions:**\n",
    "1. Instead of just using the environment's reward, modify the reward function to include a penalty for large pole angles.\n",
    "2. Update the reward calculation in the `train_dqn` function to discourage the agent from letting the pole deviate too far from the center.\n",
    "3. Observe the effect on the agent's learning and episode length.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2cd517-6d3b-4478-9307-8854690ae7cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "338867dd-58b6-4e74-a3b3-da71baa0ab6f",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click here for solution</summary> </br>\n",
    "\n",
    "```python\n",
    "import os\n",
    "\n",
    "# Create sample directory structure if it does not exist\n",
    "base_dir = 'sample_data'\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "val_dir = os.path.join(base_dir, 'validation')\n",
    "class1_train = os.path.join(train_dir, 'class1')\n",
    "class2_train = os.path.join(train_dir, 'class2')\n",
    "class1_val = os.path.join(val_dir, 'class1')\n",
    "class2_val = os.path.join(val_dir, 'class2')\n",
    "\n",
    "# Create directories if they do not exist\n",
    "for dir_path in [train_dir, val_dir, class1_train, class2_train, class1_val, class2_val]:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "print(\"Directory structure created. Add your images to these directories.\")\n",
    "\n",
    "# Import the necessary library\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Modify data generator to include validation data\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    'sample_data',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='binary',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    'sample_data',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='binary',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "# Function to modify the reward to encourage longer episodes\n",
    "def modify_reward(reward, next_state):\n",
    "    # Penalize large pole angles\n",
    "    pole_angle = abs(next_state[2])  # Extract the pole angle from the state\n",
    "    penalty = 1 if pole_angle > 0.1 else 0  # Apply penalty if angle is large\n",
    "    return reward - penalty  # Adjust reward\n",
    "\n",
    "# Inside the training loop\n",
    "# Example usage in a reinforcement learning training loop:\n",
    "# reward = modify_reward(reward, next_state)  # Use the modified reward\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef1506b-42fc-4fe5-9c6e-e74c81527e9e",
   "metadata": {},
   "source": [
    "### Exercise 2: Implement Early Stopping Based on Episode Length\n",
    "**Objective:** Stop training early if the agent consistently reaches the maximum episode length.\n",
    "\n",
    "**Instructions:**\n",
    "1. Add an early stopping mechanism that stops training if the agent achieves a specified number of consecutive episodes with a length above a threshold.\n",
    "2. Set the threshold as 195 steps for 100 consecutive episodes (for CartPole).\n",
    "3. Print a message and stop training when this condition is met.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213d2bc3-7ffb-4fea-93f7-ee722e8ec6ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e25259b7-17b7-45e3-a3d5-62a3c4e04664",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click here for solution</summary> </br>\n",
    "\n",
    "```python\n",
    "# Import necessary libraries\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Modify data generator to include validation data\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    'sample_data',  \n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='binary',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    'sample_data',  \n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='binary',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "# Early stopping parameters\n",
    "consecutive_success_threshold = 100\n",
    "success_episode_length = 195\n",
    "consecutive_success_count = 0\n",
    "episode_lengths = []  # Initialize episode lengths list\n",
    "\n",
    "# Example of training loop (this should be your actual loop)\n",
    "for episode in range(1000):  # Replace with actual loop condition\n",
    "    # Training logic goes here\n",
    "    episode_length = 200  # Example value, replace with actual calculation\n",
    "    episode_lengths.append(episode_length)\n",
    "    \n",
    "    # Early stopping check\n",
    "    if len(episode_lengths) > consecutive_success_threshold and all(\n",
    "        length >= success_episode_length for length in episode_lengths[-consecutive_success_threshold:]\n",
    "    ):\n",
    "        print(\"Early stopping: Agent consistently reaches max episode length.\")\n",
    "        break  # This break is now correctly inside the loop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e84e67-0280-4554-8c01-b7ea2fa9a141",
   "metadata": {},
   "source": [
    "### Exercise 3: Experiment with Different Exploration Strategies\n",
    "**Objective:** Implement an epsilon decay schedule that switches from linear decay to exponential decay after a certain number of episodes.\n",
    "\n",
    "**Instructions:**\n",
    "1. Modify the epsilon decay strategy to start with a linear decay until a specific episode, then switch to an exponential decay.\n",
    "2. Implement a check in the `choose_action` function to change the decay strategy after 100 episodes.\n",
    "3. Observe and compare the agent’s performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6622c8-1de4-4c35-b469-d42370029204",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0aadfbe-0a56-4cfb-8ba7-2d200ff1625b",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click here for solution</summary> </br>\n",
    "\n",
    "```python\n",
    "# Modify data generator to include validation data\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    'sample_data',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='binary',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    'sample_data',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='binary',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "def decay_epsilon(epsilon, episode, switch_episode=100):\n",
    "    if episode < switch_episode:\n",
    "        return max(epsilon - 0.01, 0.01)  # Linear decay\n",
    "    else:\n",
    "        return max(epsilon * 0.99, 0.01)  # Exponential decay\n",
    "\n",
    "# Inside the training loop\n",
    "epsilon = decay_epsilon(epsilon, e)  # Adjust epsilon based on the current episode\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "prev_pub_hash": "8f9a22d394782890b42687282f9bc5e03221d5b51569b44b9e5a5dab4e2ce49a"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
